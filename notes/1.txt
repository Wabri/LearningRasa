Rasa NLU

è uno strumento per fare il natural language understanding (NLU)

è un open source tool che permette la classificazione degli intenti e
l'estrazione delle entità usate negli intenti

prendiamo per esempio la frase: "Sto cercando un ristorante messicano in centro"
il risultato che otterremo è un json di questo tipo:
{
  "intent": "ricerca_ristorante",
  "entities": {
    "cuisine" : "messicano",
    "location" : "centro"
  }
}
quindi gli intent non sono altro che l'intento della frase, mentre le entities
sono gli oggetti della frase che possono essere utili

questo strumento quindi serve per processare i messaggi. Infatti c'è un
componente per la classificazione dell'intento e diversi componenti invece
per il riconoscimento delle entità

per installare questo strumento è necessario python e pip:
$ pip install rasa_nlu

è possibile eseguire rasa con molti backends, la scelta migliore è spacy+sklearn
per installare questi strumenti eseguiamo i comandi:
$ pip install -U spacy
$ pip install rasa_nlu[spacy]
in questo modo saranno installati rasa_nlu e spacy
ora è necessario scaricare le librerie dei linguaggi per NLU
$ python -m spacy download it
$ python -m spacy link it_core_news_sm it
(teoricamente crea già il link direttamente con il primo comando ma nel caso
esplicitare il collegamento con il secondo comando)

a questo punto possiamo fare un esempio: creare un bot per la ricerca
di ristoranti. Definiamo quindi 3 tipologie di intenti:
 - saluto
 - ricercaRistorante
 - ringraziamento
è ovvio che ci sono diversi modi per salutare:
 - ciao
 - salve
 - buongiorno
e diversi modi per richidere informazioni per un ristorante:
 - conosci qualche posto per mangiare la pizza in centro?
 - ho fame!
 - sono a nord della città e voglio mangiare messicano.
quindi la prima cosa che deve fare rasa è quello di riconoscere l'intento
del testo, nel nostro caso deve dire se preso un testo quello rientra nei casi
di intento: saluto, ricercaRistorante, ringraziamento. Subito dopo deve
riconoscere ed etichettare delle parole chiave definendo delle entità.
Per esempio se abbiamo "sono a nord della città e voglio mangiare messicano"
rasa deve capire che questo testo corrisponde all'intento di ricercaRistorante,
e che ci sono 2 entità che è possibile estrapolare che sono: "nord" che
rappresenta una posizione, "messicano" che rappresenta il tipo di cucina.

per potergli far fare questa magia è necessario allenare l'intelligenza.
Il training è fondamentale, più dati abbiamo e più intellingente sarà il nostro
bot. nel caso precedente abbiamo preso la frase "sono a nord della città e
voglio mangiare messicano" per far comprendere questa frase all'ai dobbiamo
trascriverla sotto forma di file json in questo modo:
{
  "text":"sono a nord della città e voglio mangiare messicano",
  "intent":"ricercaRistorante",
  "entities": [
  {
  "start":7,
  "end":10,
  "value":"nord",
  "entity":"posizione",
  }, {
  "start":42,
  "end":50,
  "value":"messicano",
  "entity":"cucina"
  }
  ]
}
questo è l'unico modo in cui l'intelligenza artificiale comprenderà la frase.
possiamo fare un altro esempio più semplice la frase "ciao":
{
  "text":"ciao",
  "intent":"saluto",
  "entities": [ ]
}
per facilità scarichiamo un set di dati già preimpostato:
https://github.com/RasaHQ/rasa_nlu/blob/master/data/examples/rasa/demo-rasa.json
creiamo una cartella data/examples/rasa in cui andrà il file demo-rasa.json.
copiamo incolliamo il contenuto del sito all'interno di questo file.

esiste un tool grafico per la modifica e aggiunta dei data test, è possibile
installarlo tramite la repository: https://github.com/RasaHQ/rasa-nlu-trainer
una volta installato tramite node package manager è possibile eseguirlo
direttamente nella cartella in cui si trova il file json dei training tramite
il comando omonimo:
$ rasa-nlu-trainer
che aprirà una pagina web ospitata in localhost in cui si troveranno tutti i
train attuali dell'intelligenza. usando questo strumento risulta molto più
semplice creare dei set di dati.

andremo ora a definire la configurazione del backend dell'intelligenza.
Creiamo quindi il file config_spacy.yml nella cartella di lavoro con il seguente
codice:

language: "it"

pipeline: "spacy_sklearn"

a questo punto abbiamo tutto quello di cui abbiamo bisogno per generare alcuni
modelli che il backend potrà usare, eseguiamo il comando python:
$ python -m rasa_nlu.train --config path_to_config --data path_to_data --path projects
l'indicazione config è la configurazione del modello di machine learning
l'indicazione data è il file o la cartella in cui sono contenuti i dati di
training
l'indicazione --path è l'output effettivo del modello che verrà creato
ci metterà un po' ma quando avrà finito dovremmo vedere una cartella projects in
cui avremo il nostro modello appena creato

per usare il modello creato precedentemente è necessario eseguire il comando
$ python -m rasa_nlu.server --path projects
che andrà a prendere il modello default nella cartella projects. a questo punto
facendo una chiamata get all'indirizzo localhost:5000/parse?q=ciao dovremmo
ricevere in output un file json con le informazioni del parsing fatto:
{
  "intent": {
    "name": "saluto",
    "confidence": 0.5345603412153835
  },
  "entities": [],
  "intent_ranking": [
    {
      "name": "saluto",
      "confidence": 0.5345603412153835
    },
    {
      "name": "ringraziamento",
      "confidence": 0.2604707895172954
    },
    {
      "name": "ricercaRistorante",
      "confidence": 0.20496886926732114
    }
  ],
  "text": "ciao",
  "project": "default",
  "model": "model_20180612-152415"
}

Avendo già effettuato un training molto grande da dialogflow posso estrarre
questi dati per usarli da rasa. Per farlo basterà scaricare il file zip
direttamente da dialogflow e estrarre il contenuto in una cartella, nel nostro
caso la metteremo all'interno di data/example/dialogflowData. estraendone il
contenuto otterremo 2 cartelle: entities e intents; e 2 json: agent e package.
per poter usare questi file possiamo eseguire il comando:
$ python -m rasa_nlu.train --config config/config_spacy.yml --data data/examples/rasa/ --path projectsDialogflow
questo creerà un modello come prima, ma dato che ci sono molti più dati, avremo
un modello molto più cospicuo.
a questo punto possiamo usare il nostro modello da backend eseguendo il solito
comando:
$ python -m rasa_nlu.server --path projectsDialogflow
eseguendo una richiesta "pagare mario 100 euro" avremo un output di questo tipo
{
  "intent": {
    "name": "payRequest",
    "confidence": 0.9346916129148384
  },
  "entities": [
    {
      "start": 0,
      "end": 6,
      "value": "pagare",
      "entity": "payRequest",
      "confidence": 0.9881267419312998,
      "extractor": "ner_crf"
    },
    {
      "start": 7,
      "end": 12,
      "value": "mario",
      "entity": "payToSomeone",
      "confidence": 0.9866899484313593,
      "extractor": "ner_crf"
    },
    {
      "start": 13,
      "end": 16,
      "value": "100",
      "entity": "number",
      "confidence": 0.9855790946392894,
      "extractor": "ner_crf"
    },
    {
      "start": 17,
      "end": 21,
      "value": "euro",
      "entity": "currency-name",
      "confidence": 0.9336220285338104,
      "extractor": "ner_crf"
    }
  ],
  "intent_ranking": [
    {
      "name": "payRequest",
      "confidence": 0.9346916129148384
    },
    {
      "name": "payRequest - yes",
      "confidence": 0.0363128329170857
    },
    {
      "name": "payRequest - no",
      "confidence": 0.018627551951329074
    },
    {
      "name": "Default Welcome Intent",
      "confidence": 0.010368002216746724
    }
  ],
  "text": "pagare mario 100 euro",
  "project": "default",
  "model": "model_20180612-155926"
}
che è il risultato corrispondente alla nostra richiesta.
Usando questi data training non potrà funzionare il rasa-nlu-trainer grafico,
ma useremo direttamente la console messa a disposizione da dialogflow.

il training data per Rasa nlu è strutturato in parti differenti:
common_examples, entity_synonyms e regex_features.
{
  "rasa_nlu_data" : {
    "common_examples":[],
    "regex_features":[],
    "entity_synonyms":[]
  }
}
quello più importante è quello degli esempi comuni common_examples. questo
viene usato per fare il train delle entità e degli intenti del modello.
i regex_features sono un tool per aiutare la classificazione delle entità o
degli intenti, aumentando la performance.
per la creazione di questi dataset esistono molti tool come:
tracy (https://yuukanoo.github.io/tracy) e
Chatito (https://rodrigopivi.github.io/Chatito/)

i common_examples ha 3 componenti:
 - testo, è un esempio di come dovrebbe presentarsi la frase dove eseguire il
 parsing;
 - intento, è l'intento (scopo) associato al testo;
 - entità, sono parti specifiche nel test a cui è possibile associare un
 identificatore.
i primi 2 sono stringhe mentre l'ultimo è un array.

la parte di entity_synonyms è la parte in cui vengono indicati i sinonimi che è
possibile trovare in una frase. per esempio la parola "pagare" nel nostro caso
ha lo stesso significato di "paga" dato che la raggruppiamo nello stesso gruppo
di entità "payRequest". un altro esempio generico può essere "New York" e "NY"
entrambi hanno lo stesso significato corrispondente all'entità luogo che non
differenzierà tra le due parole e gli assegnerà un unico valore:
{
  "rasa_nlu_data": {
    "entity_synonyms": [
      {
        "value": "New York",
        "synonyms": ["NY", "ny", "the big apple"]
      }
    ]
  }
}

le regex_features non sono altro che le regular expression che possono essere
usate come supporto alla classificazione degli intenti e delle entità. per
esempio un codice postale (chiamato anche zipcode) è sempre formato da 5 numeri
compresi tra 0 e 9, che possiamo rappresentare nella nostra configurazione come:
{
    "rasa_nlu_data": {
        "regex_features": [
            {
                "name": "zipcode",
                "pattern": "[0-9]{5}"
            },
            {
                "name": "greet",
                "pattern": "hey[^\\s]*"
            },
        ]
    }
}

i training data possono essere salvati in singoli file o in file diversi.
per grandi progetti con molti intenti e entità questo migliora la mantenibilità
perchè è possibile dividere in file diversi i training di diversi intenti,
invece di mantenerli su un singolo file.

è possibile eseguire un server http che riceva le richieste usando il comando:
$ python -m rasa_nlu.server --path projects
il server andrà a guardare se esistono dei progetti sotto la cartella path
indicata dal parametro, altrimenti di default andrà a prendere l'ultimo modello
di training. il server può emulare i servizi: wit, luis e dialogflow. in questo
modo se esportiamo a partire da questi servizi la sintassi rimarrà la solita.
per farlo basta inserire il parametro emulate indicando il tipo di emulazione:
$ python -m rasa_nlu.server --path projects --emulate dialogflow
una volta avviato il server è possibile usarlo da un endpoint tramite richieste
post:
 - parse, restituisce il risultato del parsing di una frase:
    http://localhost:5000/parse?q=<frase da parsare>
 - version, restituisce la versione del modello usato
 - config, restituisce la configurazione attualmente in uso
 - status, restituisce lo stato attuale del server
per proteggere il server è possibile specificare un token nelle configurazioni
di rasa, aggiungendo "token":"12345" al file di configurazione o settando
RASA_TOKEN nelle variabili d'ambiente. se è stato settato allora per tutte le
richieste sarà necessario la specifica del token, per esempio nella richiesta
status:
localhost:5000/status?token=12345
questo è necessario quando si vuole chiamare il server da un altro dominio (per
esempio da un interfaccia training web), per rendere possibile la trasmissione
di dati sarà necessario aggiungere in whitelist il dominio nel cors_origin.
il cors origin sono le configurazioni di CORS (cross origin resource sharing).
