Rasa NLU

è uno strumento per fare il natural language understanding (NLU)

è un open source tool che permette la classificazione degli intenti e
l'estrazione delle entità usate negli intenti

prendiamo per esempio la frase: "Sto cercando un ristorante messicano in centro"
il risultato che otterremo è un json di questo tipo:
{
  "intent": "ricerca_ristorante",
  "entities": {
    "cuisine" : "messicano",
    "location" : "centro"
  }
}
quindi gli intent non sono altro che l'intento della frase, mentre le entities
sono gli oggetti della frase che possono essere utili

questo strumento quindi serve per processare i messaggi. Infatti c'è un
componente per la classificazione dell'intento e diversi componenti invece
per il riconoscimento delle entità

per installare questo strumento è necessario python e pip:
$ pip install rasa_nlu

è possibile eseguire rasa con molti backends, la scelta migliore è spacy+sklearn
per installare questi strumenti eseguiamo i comandi:
$ pip install -U spacy
$ pip install rasa_nlu[spacy]
in questo modo saranno installati rasa_nlu e spacy
ora è necessario scaricare le librerie dei linguaggi per NLU
$ python -m spacy download it
$ python -m spacy link it_core_news_sm it
(teoricamente crea già il link direttamente con il primo comando ma nel caso
esplicitare il collegamento con il secondo comando)

a questo punto possiamo fare un esempio: creare un bot per la ricerca
di ristoranti. Definiamo quindi 3 tipologie di intenti:
 - saluto
 - ricercaRistorante
 - ringraziamento
è ovvio che ci sono diversi modi per salutare:
 - ciao
 - salve
 - buongiorno
e diversi modi per richidere informazioni per un ristorante:
 - conosci qualche posto per mangiare la pizza in centro?
 - ho fame!
 - sono a nord della città e voglio mangiare messicano.
quindi la prima cosa che deve fare rasa è quello di riconoscere l'intento
del testo, nel nostro caso deve dire se preso un testo quello rientra nei casi
di intento: saluto, ricercaRistorante, ringraziamento. Subito dopo deve
riconoscere ed etichettare delle parole chiave definendo delle entità.
Per esempio se abbiamo "sono a nord della città e voglio mangiare messicano"
rasa deve capire che questo testo corrisponde all'intento di ricercaRistorante,
e che ci sono 2 entità che è possibile estrapolare che sono: "nord" che
rappresenta una posizione, "messicano" che rappresenta il tipo di cucina.

per potergli far fare questa magia è necessario allenare l'intelligenza.
Il training è fondamentale, più dati abbiamo e più intellingente sarà il nostro
bot. nel caso precedente abbiamo preso la frase "sono a nord della città e
voglio mangiare messicano" per far comprendere questa frase all'ai dobbiamo
trascriverla sotto forma di file json in questo modo:
{
  "text":"sono a nord della città e voglio mangiare messicano",
  "intent":"ricercaRistorante",
  "entities": [
  {
  "start":7,
  "end":10,
  "value":"nord",
  "entity":"posizione",
  }, {
  "start":42,
  "end":50,
  "value":"messicano",
  "entity":"cucina"
  }
  ]
}
questo è l'unico modo in cui l'intelligenza artificiale comprenderà la frase.
possiamo fare un altro esempio più semplice la frase "ciao":
{
  "text":"ciao",
  "intent":"saluto",
  "entities": [ ]
}
per facilità scarichiamo un set di dati già preimpostato:
https://github.com/RasaHQ/rasa_nlu/blob/master/data/examples/rasa/demo-rasa.json
creiamo una cartella data/examples/rasa in cui andrà il file demo-rasa.json.
copiamo incolliamo il contenuto del sito all'interno di questo file.

esiste un tool grafico per la modifica e aggiunta dei data test, è possibile
installarlo tramite la repository: https://github.com/RasaHQ/rasa-nlu-trainer
una volta installato tramite node package manager è possibile eseguirlo
direttamente nella cartella in cui si trova il file json dei training tramite
il comando omonimo:
$ rasa-nlu-trainer
che aprirà una pagina web ospitata in localhost in cui si troveranno tutti i
train attuali dell'intelligenza. usando questo strumento risulta molto più
semplice creare dei set di dati.

andremo ora a definire la configurazione del backend dell'intelligenza.
Creiamo quindi il file config_spacy.yml nella cartella di lavoro con il seguente
codice:

language: "it"

pipeline: "spacy_sklearn"

a questo punto abbiamo tutto quello di cui abbiamo bisogno per generare alcuni
modelli che il backend potrà usare, eseguiamo il comando python:
$ python -m rasa_nlu.train --config path_to_config --data path_to_data --path projects
l'indicazione config è la configurazione del modello di machine learning
l'indicazione data è il file o la cartella in cui sono contenuti i dati di
training
l'indicazione --path è l'output effettivo del modello che verrà creato
ci metterà un po' ma quando avrà finito dovremmo vedere una cartella projects in
cui avremo il nostro modello appena creato

per usare il modello creato precedentemente è necessario eseguire il comando
$ python -m rasa_nlu.server --path projects
che andrà a prendere il modello default nella cartella projects. a questo punto
facendo una chiamata get all'indirizzo localhost:5000/parse?q=ciao dovremmo
ricevere in output un file json con le informazioni del parsing fatto:
{
  "intent": {
    "name": "saluto",
    "confidence": 0.5345603412153835
  },
  "entities": [],
  "intent_ranking": [
    {
      "name": "saluto",
      "confidence": 0.5345603412153835
    },
    {
      "name": "ringraziamento",
      "confidence": 0.2604707895172954
    },
    {
      "name": "ricercaRistorante",
      "confidence": 0.20496886926732114
    }
  ],
  "text": "ciao",
  "project": "default",
  "model": "model_20180612-152415"
}

Avendo già effettuato un training molto grande da dialogflow posso estrarre
questi dati per usarli da rasa. Per farlo basterà scaricare il file zip
direttamente da dialogflow e estrarre il contenuto in una cartella, nel nostro
caso la metteremo all'interno di data/example/dialogflowData. estraendone il
contenuto otterremo 2 cartelle: entities e intents; e 2 json: agent e package.
per poter usare questi file possiamo eseguire il comando:
$ python -m rasa_nlu.train --config config/config_spacy.yml --data data/examples/rasa/ --path projectsDialogflow
questo creerà un modello come prima, ma dato che ci sono molti più dati, avremo
un modello molto più cospicuo.
a questo punto possiamo usare il nostro modello da backend eseguendo il solito
comando:
$ python -m rasa_nlu.server --path projectsDialogflow
eseguendo una richiesta "pagare mario 100 euro" avremo un output di questo tipo
{
  "intent": {
    "name": "payRequest",
    "confidence": 0.9346916129148384
  },
  "entities": [
    {
      "start": 0,
      "end": 6,
      "value": "pagare",
      "entity": "payRequest",
      "confidence": 0.9881267419312998,
      "extractor": "ner_crf"
    },
    {
      "start": 7,
      "end": 12,
      "value": "mario",
      "entity": "payToSomeone",
      "confidence": 0.9866899484313593,
      "extractor": "ner_crf"
    },
    {
      "start": 13,
      "end": 16,
      "value": "100",
      "entity": "number",
      "confidence": 0.9855790946392894,
      "extractor": "ner_crf"
    },
    {
      "start": 17,
      "end": 21,
      "value": "euro",
      "entity": "currency-name",
      "confidence": 0.9336220285338104,
      "extractor": "ner_crf"
    }
  ],
  "intent_ranking": [
    {
      "name": "payRequest",
      "confidence": 0.9346916129148384
    },
    {
      "name": "payRequest - yes",
      "confidence": 0.0363128329170857
    },
    {
      "name": "payRequest - no",
      "confidence": 0.018627551951329074
    },
    {
      "name": "Default Welcome Intent",
      "confidence": 0.010368002216746724
    }
  ],
  "text": "pagare mario 100 euro",
  "project": "default",
  "model": "model_20180612-155926"
}
che è il risultato corrispondente alla nostra richiesta.

(https://nlu.rasa.com/dataformat.html)
